{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H0vPuJc0Bdda"
   },
   "source": [
    "# Ex. 3. Experimental architecture comparison\n",
    "\n",
    "### This notebook uses the template provided by Christian Igel, for the German Traffic Sign Recognition Benchmark. \n",
    "The notebook generates 4 different architectures (original, dropout1, dropout2, dropout3), 6 models for each architecture, for a total of 24 models (description in the report). It train the models and generate a plot visualizing the training progress of each model architecture. It uses each model for prediction, build a dataframe for each model architecture performance. Compute the mean and median of the loss and the accuracy on the test data. Finally, it generate two boxplots, one for the accuracy and the other for the loss, visualizing the difference between the classification performance of the model architectures.  \n",
    " \n",
    "Since the notebook train and evaluate 24 models, it takes around 7 hours to run on my local machine (it takes much more on Google Colab). I generated this notebook only for providing the code of this exercise as a single file. Initially, I simply generated different models, trained and saved them to my locale machine, each one on a different notebook. Then, I loaded them into a single notebook and I computed the predictions, the evaluation and I generated the plots. But since it has been requested to provide the code for the exercises, I decided to provide a compact code in a single notebook able to generate the data I used in the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "5YkvUiAmRo5o",
    "outputId": "d09e6837-18cd-4af6-b677-4cc7161b76ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.2.0\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline \n",
    "%tensorflow_version 2.x\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "import pathlib\n",
    "import os\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q979j4s3YfLU"
   },
   "source": [
    "Download the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bNKS8W-NRo5v"
   },
   "outputs": [],
   "source": [
    "data_root=\".\"\n",
    "tf.keras.utils.get_file(\"GTSRB.zip\",\n",
    "                        \"https://sid.erda.dk/share_redirect/EB0rrpZwuI\",\n",
    "                        cache_dir=data_root,\n",
    "                        extract=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vH0IthqdcLyl"
   },
   "source": [
    "The images are stored in subdirectories. The names of the subdirectories encode the class labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sFCdlu-RTxTn"
   },
   "outputs": [],
   "source": [
    "# Determine the number of training and test images\n",
    "n_train = len(list(pathlib.Path(data_root).glob(\"datasets/GTSRB/train/*/*\")))\n",
    "n_test = len(list(pathlib.Path(data_root).glob(\"datasets/GTSRB/test/*/*\")))\n",
    "\n",
    "print(\"Number of training images:\", n_train)\n",
    "print(\"Number of test images:\" ,n_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "91MRbgaPgU5Y"
   },
   "source": [
    "Some basic constants: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h7FDm0TtRo5s"
   },
   "outputs": [],
   "source": [
    "no_classes = 43  \n",
    "no_channels = 3  \n",
    "\n",
    "# All images are initially resized to img_height x img_width\n",
    "img_height  = 32  \n",
    "img_width   = 32\n",
    "# During training and testing the  images are cropped to img_height_crop x img_width_crop\n",
    "img_height_crop = 28  \n",
    "img_width_crop  = 28\n",
    "\n",
    "batch_size = 128\n",
    "steps_per_epoch = n_train // batch_size  # How many batches are there in each epoch?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rq4mr2pea6Ap"
   },
   "source": [
    "Helper function for extracting the label information from the paths to the images and loading and preprocessing the images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y5IhhN-IRo5x"
   },
   "outputs": [],
   "source": [
    "# Extract the label from the file path\n",
    "def get_label(file_path):\n",
    "  # convert the path to a list of path components\n",
    "  parts = tf.strings.split(file_path, os.path.sep)\n",
    "  # The second to last is the class-directory\n",
    "  return tf.strings.to_number(parts[-2], tf.int32)\n",
    "\n",
    "# Load image, convert it to floats, and resize it\n",
    "def decode_img(img):\n",
    "  # convert the compressed string to a 3D uint8 tensor\n",
    "  img = tf.image.decode_png(img, channels=no_channels)\n",
    "  # Use `convert_image_dtype` to convert to floats in the [0,1] range.\n",
    "  img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "  # Resize the image to the desired size.\n",
    "  return tf.image.resize(img, [img_width, img_height])\n",
    "\n",
    "# Given the path and filename of an image, create the label and the input image\n",
    "def process_path(file_path):\n",
    "  label = get_label(file_path)\n",
    "  # Load the raw data from the file as a string\n",
    "  img = tf.io.read_file(file_path)\n",
    "  img = decode_img(img)\n",
    "  return img, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "suZH5kFsayst"
   },
   "source": [
    "Create data sets based on the files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dGyT5M25Ro5z"
   },
   "outputs": [],
   "source": [
    "# Tell dataset the lists of files containing the trainig and test images, respectively\n",
    "list_ds_train = tf.data.Dataset.list_files(data_root + \"/datasets/GTSRB/train/*/*\")\n",
    "list_ds_test  = tf.data.Dataset.list_files(data_root + \"/datasets/GTSRB/test/*/*\", shuffle=False)  # Fixed order for test time augemantation \n",
    "\n",
    "# Tell dataset how to extract images and labels\n",
    "labeled_ds_train = list_ds_train.map(process_path, num_parallel_calls=AUTOTUNE) # num of parallel processing depends on the CPU core\n",
    "labeled_ds_test = list_ds_test.map(process_path, num_parallel_calls=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ee-DezPHgyA-"
   },
   "source": [
    "Functions for data preprocessing/augmentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A8usLWHERo56"
   },
   "outputs": [],
   "source": [
    "# Used for training data augmentation\n",
    "def augment(image, label):\n",
    "    # Take a random sub-image 28x28 from the input\n",
    "    image = tf.image.random_crop(image, [img_height_crop, img_width_crop, no_channels])\n",
    "    # Only for the defines labels, random flip the image\n",
    "    image = tf.case([(tf.equal(label, 11), lambda: tf.image.random_flip_left_right(image)),\n",
    "                     (tf.equal(label, 12), lambda: tf.image.random_flip_left_right(image)),\n",
    "                     (tf.equal(label, 13), lambda: tf.image.random_flip_left_right(image)),\n",
    "                     (tf.equal(label, 17), lambda: tf.image.random_flip_left_right(image)),\n",
    "                     (tf.equal(label, 18), lambda: tf.image.random_flip_left_right(image)),\n",
    "                     (tf.equal(label, 26), lambda: tf.image.random_flip_left_right(image)),\n",
    "                     (tf.equal(label, 30), lambda: tf.image.random_flip_left_right(image)),\n",
    "                     (tf.equal(label, 35), lambda: tf.image.random_flip_left_right(image))], default = lambda: image)\n",
    "    # Change the brightness (sometimes brighter, sometimes darker)\n",
    "    image = tf.image.random_brightness(image, max_delta=0.1) # Random brightness\n",
    "    # Clip all values from 0 and 1, so that they are the values of a proper image\n",
    "    image = tf.clip_by_value(image, 0., 1.)\n",
    "    return image, label\n",
    "\n",
    "# Used for testing/evaluation\n",
    "def crop_center(image, label):\n",
    "  # I take the center of the image 28x28\n",
    "  image = tf.image.resize_with_crop_or_pad(image, img_height_crop, img_width_crop)\n",
    "  return image, label\n",
    "\n",
    "# Could be used for test time augementation\n",
    "def crop_random(image, label):\n",
    "  image = tf.image.random_crop(image, [img_height_crop, img_width_crop, no_channels])\n",
    "  return image, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CXJr7YbkhzM-"
   },
   "source": [
    "We prepare the data for training and testing differently. For example, for training we use data augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kwMKlQNRRo58"
   },
   "outputs": [],
   "source": [
    "def prepare_for_training(ds, cache=True, shuffle_buffer_size=1000):\n",
    "  # `.cache(filename)` is used to cache preprocessing work for datasets that don't\n",
    "  # fit in memory.\n",
    "  if cache:\n",
    "    if isinstance(cache, str):          \n",
    "      ds = ds.cache(cache)             \n",
    "    else:\n",
    "      ds = ds.cache()  \n",
    "  # We shuffle the data after the caching\n",
    "  ds = ds.shuffle(buffer_size=shuffle_buffer_size, reshuffle_each_iteration=True)\n",
    "  # Repeat forever\n",
    "  ds = ds.repeat()                    \n",
    "  # Do data augmentation\n",
    "  ds = ds.map(augment, num_parallel_calls=AUTOTUNE) \n",
    "  # Partition in batches\n",
    "  ds = ds.batch(batch_size)\n",
    "  # Fetch batches in the background while the model training\n",
    "  ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
    "  return ds\n",
    "\n",
    "def prepare_for_evaluation(ds, shuffle_buffer_size=1000):\n",
    "  # For normal evaluation, we look at the center of the image\n",
    "  ds = ds.map(crop_center, num_parallel_calls=AUTOTUNE)        \n",
    "  # Partition in batches\n",
    "  ds = ds.batch(batch_size)\n",
    "  # Fetch batches in the background while the model training\n",
    "  ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
    "  return ds\n",
    "\n",
    "def prepare_for_augmented_evaluation(ds):\n",
    "  ds = ds.map(crop_random, num_parallel_calls=AUTOTUNE)\n",
    "  ds = ds.batch(batch_size)\n",
    "  ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
    "  return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pqHJB7tioo_3"
   },
   "source": [
    "Let's have a look at a random batch of images:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mmnOiBr5ojsU"
   },
   "outputs": [],
   "source": [
    "# Helper function for displaying images\n",
    "def show_batch(image_batch, label_batch, nrows=6, ncols=6):\n",
    "  plt.figure(figsize=(10,10))\n",
    "  for n in range(nrows*ncols):\n",
    "      ax = plt.subplot(nrows, ncols, n+1)\n",
    "      if no_channels == 3:\n",
    "        plt.imshow(image_batch[n])\n",
    "      else:\n",
    "        plt.imshow(image_batch[n].reshape([img_height_crop, img_width_crop]))\n",
    "      plt.title('class:' + str(label_batch[n]))\n",
    "      plt.axis('off')\n",
    "      \n",
    "# Make training dataset \n",
    "train_ds = prepare_for_training(labeled_ds_train)\n",
    "\n",
    "# Make training dataset \n",
    "test_ds = prepare_for_evaluation(labeled_ds_test)\n",
    "\n",
    "# Get a batch of images and labels\n",
    "image_batch, label_batch = next(iter(train_ds))\n",
    "\n",
    "# Visualize images and labels\n",
    "show_batch(image_batch.numpy(), label_batch.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yHW1Yk3pjym9"
   },
   "source": [
    "Functions to generate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m_V9IihwRo6E"
   },
   "outputs": [],
   "source": [
    "sd_init = 0.01  \n",
    "\n",
    "# Original-model\n",
    "def original_generator(): \n",
    "  return tf.keras.Sequential([\n",
    "            tf.keras.layers.Conv2D(32, (5, 5), activation=None,  \n",
    "                                  input_shape=(img_width_crop, img_height_crop, no_channels),\n",
    "                                  bias_initializer=tf.initializers.TruncatedNormal(mean=sd_init, stddev=sd_init)),\n",
    "            tf.keras.layers.ELU(),\n",
    "            tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "            tf.keras.layers.Conv2D(64, (5, 5), activation=None, \n",
    "                                  bias_initializer=tf.initializers.TruncatedNormal(mean=sd_init, stddev=sd_init)),\n",
    "            tf.keras.layers.ELU(),\n",
    "            tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(no_classes, activation='softmax')])\n",
    "  \n",
    "# Dropout1-model\n",
    "def dropout1_generator(): \n",
    "  return tf.keras.Sequential([\n",
    "            tf.keras.layers.Conv2D(32, (5, 5), activation=None,  \n",
    "                                  input_shape=(img_width_crop, img_height_crop, no_channels),\n",
    "                                  bias_initializer=tf.initializers.TruncatedNormal(mean=sd_init, stddev=sd_init)),\n",
    "            tf.keras.layers.ELU(),\n",
    "            tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "            tf.keras.layers.Dropout(0.5),\n",
    "            tf.keras.layers.Conv2D(64, (5, 5), activation=None, \n",
    "                                  bias_initializer=tf.initializers.TruncatedNormal(mean=sd_init, stddev=sd_init)),\n",
    "            tf.keras.layers.ELU(),\n",
    "            tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "            tf.keras.layers.Dropout(0.5),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(no_classes, activation='softmax')])\n",
    "\n",
    "# Dropout2-model\n",
    "def dropout2_generator(): \n",
    "  return tf.keras.Sequential([\n",
    "            tf.keras.layers.Conv2D(32, (5, 5), activation=None,  \n",
    "                                  input_shape=(img_width_crop, img_height_crop, no_channels),\n",
    "                                  bias_initializer=tf.initializers.TruncatedNormal(mean=sd_init, stddev=sd_init)),\n",
    "            tf.keras.layers.ELU(),\n",
    "            tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "            tf.keras.layers.Conv2D(64, (5, 5), activation=None, \n",
    "                                  bias_initializer=tf.initializers.TruncatedNormal(mean=sd_init, stddev=sd_init)),\n",
    "            tf.keras.layers.ELU(),\n",
    "            tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dropout(0.5),\n",
    "            tf.keras.layers.Dense(no_classes, activation='softmax')])\n",
    "\n",
    "# Dropout3-model\n",
    "def dropout3_generator(): \n",
    "  return tf.keras.Sequential([\n",
    "            tf.keras.layers.Conv2D(32, (5, 5), activation=None,  \n",
    "                                  input_shape=(img_width_crop, img_height_crop, no_channels),\n",
    "                                  bias_initializer=tf.initializers.TruncatedNormal(mean=sd_init, stddev=sd_init)),\n",
    "            tf.keras.layers.ELU(),\n",
    "            tf.keras.layers.Dropout(0.5),\n",
    "            tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "            tf.keras.layers.Conv2D(64, (5, 5), activation=None, \n",
    "                                  bias_initializer=tf.initializers.TruncatedNormal(mean=sd_init, stddev=sd_init)),\n",
    "            tf.keras.layers.ELU(),\n",
    "            tf.keras.layers.Dropout(0.5),\n",
    "            tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(no_classes, activation='softmax')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mmQXKGehyFW9"
   },
   "source": [
    "Save models architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rcAOslKmv2Ci"
   },
   "outputs": [],
   "source": [
    "# Save 6 models for the original architecture\n",
    "original1 = original_generator()\n",
    "original2 = original_generator()\n",
    "original3 = original_generator()\n",
    "original4 = original_generator()\n",
    "original5 = original_generator()\n",
    "original6 = original_generator()\n",
    "original1.summary()\n",
    "original_models = [original1, original2, original3, \n",
    "                   original4, original5, original6]\n",
    "\n",
    "# Save 6 models for the dropout1 architecture\n",
    "dropout1_1 = dropout1_generator()\n",
    "dropout1_2 = dropout1_generator()\n",
    "dropout1_3 = dropout1_generator()\n",
    "dropout1_4 = dropout1_generator()\n",
    "dropout1_5 = dropout1_generator()\n",
    "dropout1_6 = dropout1_generator()\n",
    "dropout1_1.summary()\n",
    "dropout1_models = [dropout1_2, dropout1_2, dropout1_3,\n",
    "                   dropout1_4, dropout1_5, dropout1_6]\n",
    "\n",
    "# Save 6 models for the dropout2 architecture\n",
    "dropout2_1 = dropout2_generator()\n",
    "dropout2_2 = dropout2_generator()\n",
    "dropout2_3 = dropout2_generator()\n",
    "dropout2_4 = dropout2_generator()\n",
    "dropout2_5 = dropout2_generator()\n",
    "dropout2_6 = dropout2_generator()\n",
    "dropout2_1.summary()\n",
    "dropout2_models = [dropout2_1, dropout2_2, dropout2_3,\n",
    "                   dropout2_4, dropout2_5, dropout2_6]\n",
    "\n",
    "# Save 6 models for the dropout3 architecture\n",
    "dropout3_1 = dropout3_generator()\n",
    "dropout3_2 = dropout3_generator()\n",
    "dropout3_3 = dropout3_generator()\n",
    "dropout3_4 = dropout3_generator()\n",
    "dropout3_5 = dropout3_generator()\n",
    "dropout3_6 = dropout3_generator()\n",
    "dropout3_1.summary()\n",
    "dropout3_models = [dropout3_1, dropout3_2, dropout3_3,\n",
    "                   dropout3_4, dropout3_5, dropout3_6]\n",
    "\n",
    "# Store the lists of models in a single list\n",
    "model_architectures = [original_models, dropout1_models, \n",
    "                       dropout2_models, dropout3_models]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZgNrQ6n2j-yG"
   },
   "source": [
    "Define optimizer and compile the models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3sTjGJpJRo6F"
   },
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam(learning_rate=0.001, epsilon=0.1)\n",
    "\n",
    "for architecture in model_architectures:\n",
    "  for model in architecture:\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=opt, \n",
    "                  metrics=['accuracy']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H9AXTUfVj6QG"
   },
   "source": [
    "Do the learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x_Z_aenJRo6H"
   },
   "outputs": [],
   "source": [
    "# Train the models\n",
    "history_list = []\n",
    "\n",
    "# Use validation only for the first model of each architecture\n",
    "for architecture in model_architectures:\n",
    "  i = 1\n",
    "  for model in architecture:\n",
    "    if i == 1:\n",
    "      history = model.fit(train_ds, epochs=800, steps_per_epoch=steps_per_epoch, \n",
    "                          validation_freq=1, validation_data=test_ds)\n",
    "      history_list.append(history)\n",
    "    else:\n",
    "      model.fit(train_ds, epochs=800, steps_per_epoch=steps_per_epoch)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bPpwvIaXbAjo"
   },
   "source": [
    "Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "phFUcN1R4XIv"
   },
   "outputs": [],
   "source": [
    "# Summarize history for accuracy  \n",
    "def accuracy_plot(i_arch, model_name):   \n",
    "  \"\"\"\n",
    "  i_arch : integer (0 = original, 1 = dropout1, etc)\n",
    "  model_name : sring (\"Original\", \"Dropout1\", etc)\n",
    "  \"\"\"                 \n",
    "  plt.plot(history_list[i_arch].history['accuracy'],zorder=2)\n",
    "  plt.plot(history_list[i_arch].history['val_accuracy'])\n",
    "  plt.title(model_name + '-model accuracy')\n",
    "  plt.yticks(np.arange(0.1, 1.1, 0.1))\n",
    "  plt.ylim(0,1.05)\n",
    "  plt.grid(zorder=0, color=\"lightgray\")\n",
    "  plt.ylabel('Accuracy')\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.legend(['train', 'test'], loc='lower right')\n",
    "  plt.savefig(model_name + \"_val_acc.png\", dpi=300)\n",
    "  plt.show()\n",
    "\n",
    "# Summarize history for loss\n",
    "def loss_plot(i_arch, model_name):\n",
    "  \"\"\"\n",
    "  i_arch : integer (0 = original, 1 = dropout1, etc)\n",
    "  model_name : sring (\"Original\", \"Dropout1\", etc)\n",
    "  \"\"\"  \n",
    "  plt.plot(history_list[i_arch].history['loss'],zorder=2)\n",
    "  plt.plot(history_list[i_arch].history['val_loss'])\n",
    "  plt.title(model_name + '-model loss')\n",
    "  plt.yticks(np.arange(0, 4, 0.5))\n",
    "  plt.ylim(-0.15,3.8)\n",
    "  plt.grid(zorder=0, color=\"lightgray\")\n",
    "  plt.ylabel('Loss')\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.legend(['train', 'test'], loc='upper right')\n",
    "  plt.savefig(model_name + \"_val_loss.png\", dpi=300)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NLEoZHlF7yO9"
   },
   "outputs": [],
   "source": [
    "# Save and visualize the plot of the training progress of the first model of each architecture\n",
    "architectures_names = [\"Original\", \"Dropout1\", \"Dropout2\", \"Dropout3\"]\n",
    "for i, name in enumerate(architectures_names):\n",
    "  accuracy_plot(i, name)\n",
    "  loss_plot(i, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BtDhFmaP_K0k"
   },
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "\n",
    "eval_train_ds = prepare_for_evaluation(labeled_ds_train)\n",
    "eval_test_ds = prepare_for_evaluation(labeled_ds_test)\n",
    "\n",
    "def evaluate(model):\n",
    "  test_result = model.evaluate(eval_test_ds)\n",
    "  train_result = model.evaluate(eval_train_ds)\n",
    "  return test_result, train_result\n",
    "\n",
    "# Function to generate a dataframe with the evaluation of the model\n",
    "def get_evaluation_df(models_list):\n",
    "  print(\"Evaluating model architecture..\")\n",
    "  df = np.array(evaluate(models_list[0])).reshape(1,4)\n",
    "  for i in range(1, 6):\n",
    "    temp_df = np.array(evaluate(models_list[i])).reshape(1,4)\n",
    "    df = np.concatenate((df, temp_df), axis=0) \n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8HbN7lamIG7n"
   },
   "outputs": [],
   "source": [
    "# Store the accuracy and loss on train and test set into dataframes\n",
    "original_df = get_evaluation_df(original_models)\n",
    "dropout1_df = get_evaluation_df(dropout1_models)\n",
    "dropout2_df = get_evaluation_df(dropout2_models)\n",
    "dropout3_df = get_evaluation_df(dropout3_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9h6dGi49Itg0"
   },
   "outputs": [],
   "source": [
    "# Print the evaluation\n",
    "def print_eval(model_name, evaluation_df):\n",
    "  print(\">> \" + model_name + \"-model <<\\n\")\n",
    "  print(\"Mean:\\n  test loss,  test acc, train loss, train acc\\n\", \n",
    "        np.mean(evaluation_df, axis=0),\"\\n\")\n",
    "  print(\"Median:\\n  test loss,  test acc, train loss, train acc\\n\", \n",
    "        np.median(evaluation_df, axis=0))\n",
    "  print(\"\\nAll trials:\")\n",
    "  print(\"  Test loss, Test acc,  Train loss, Train acc\")\n",
    "  print(evaluation_df, \"\\n\")\n",
    "\n",
    "print_eval(\"Original\", original_df)\n",
    "print_eval(\"Dropout1\", dropout1_df)\n",
    "print_eval(\"Dropout2\", dropout2_df)\n",
    "print_eval(\"Dropout3\", dropout3_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nK4WfQx7fQJW"
   },
   "outputs": [],
   "source": [
    "# Plot accuracy and loss performance on test set\n",
    "\n",
    "# Boxplot test accuracy\n",
    "def boxplot_acc(is_train_complete = True):\n",
    "  fig = plt.figure()\n",
    "  ax = fig.add_subplot(111)\n",
    "  medianprops = dict(color=\"red\")\n",
    "  labels = [\"Original\", \"Dropout1\", \"Dropout2\", \"Dropout3\"]\n",
    "  box = ax.boxplot([original_df[:,1], dropout1_df[:,1], \n",
    "                    dropout2_df[:,1], dropout3_df[:,1]], \n",
    "                  labels = labels, \n",
    "                  patch_artist = True, \n",
    "                  sym = 'x',\n",
    "                  medianprops = medianprops)\n",
    "  colors = ['#1f77b4', 'lightsteelblue', \"mediumseagreen\", \"plum\"]\n",
    "  for patch, color in zip(box['boxes'], colors):\n",
    "      patch.set_facecolor(color)\n",
    "  ax.set_title(\"Original model vs dropouts models test accuracy\")\n",
    "  if is_train_complete == True:\n",
    "    ax.set_ylim(0.935, 0.985)\n",
    "    major_ticks = np.arange(0.94, 0.98, 0.01)\n",
    "    minor_ticks = np.arange(0.94, 0.98, 0.005)\n",
    "    ax.set_yticks(major_ticks)\n",
    "    ax.set_yticks(minor_ticks, minor=True)\n",
    "    plt.grid(which = \"major\", axis = \"y\", alpha = 0.5)\n",
    "    plt.grid(which = \"minor\", axis = \"y\", alpha = 0.5)\n",
    "  else:\n",
    "    plt.grid(axis = \"y\", alpha = 0.5)  \n",
    "  ax.set_ylabel(\"Test accuracy\", fontsize = 12)  \n",
    "  plt.savefig(\"test_acc_boxplot.png\", dpi = 300)\n",
    "  plt.show()\n",
    "\n",
    "# Boxplot test loss\n",
    "def boxplot_loss(is_train_complete = True):\n",
    "  fig = plt.figure()\n",
    "  ax = fig.add_subplot(111)\n",
    "  medianprops = dict(color=\"red\")\n",
    "  labels = architectures_names\n",
    "  box = ax.boxplot([original_df[:,0], dropout1_df[:,0], \n",
    "                    dropout2_df[:,0], dropout3_df[:,0]], \n",
    "                  labels = labels, \n",
    "                  patch_artist = True, \n",
    "                  sym = 'x',\n",
    "                  medianprops = medianprops)\n",
    "  colors = ['#1f77b4', 'lightsteelblue', \"mediumseagreen\", \"plum\"]\n",
    "  for patch, color in zip(box['boxes'], colors):\n",
    "      patch.set_facecolor(color)\n",
    "  ax.set_title(\"Original model vs dropouts models test loss\")\n",
    "  if is_train_complete == True:\n",
    "    ax.set_ylim(0.5, 0.75)\n",
    "    major_ticks = np.arange(0.1, 0.75, 0.1)\n",
    "    minor_ticks = np.arange(0.05, 0.75, 0.05)\n",
    "    ax.set_yticks(major_ticks)\n",
    "    ax.set_yticks(minor_ticks, minor=True)\n",
    "    plt.grid(which = \"major\", axis = \"y\", alpha = 0.5)\n",
    "    plt.grid(which = \"minor\", axis = \"y\", alpha = 0.5)\n",
    "  else:\n",
    "    plt.grid(axis = \"y\", alpha = 0.5) \n",
    "  ax.set_ylabel(\"Test loss\", fontsize = 12)  \n",
    "  \n",
    "  plt.savefig(\"test_loss_boxplot\", dpi = 300)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yyARru_OVn0Q"
   },
   "outputs": [],
   "source": [
    "# If training is completed for all 800 epochs\n",
    "boxplot_acc(is_train_complete=True)\n",
    "boxplot_loss(is_train_complete=True)\n",
    "\n",
    "# # If training is not completed for all 800 epochs, just for testing the code\n",
    "# boxplot_acc(is_train_complete=False)\n",
    "# boxplot_loss(is_train_complete=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "HW2_exercise3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
